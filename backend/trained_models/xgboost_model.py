# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/194CF-sEAjeXyqliRW6S1Sl83A5nlGDy-

0) 환경 준비
"""

!pip -q install xgboost==2.0.3

import os, math, random, pickle
import numpy as np
import pandas as pd
from collections import Counter
from datetime import datetime, timedelta

import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix,
    f1_score, accuracy_score
)

import xgboost as xgb

SEED = 42
random.seed(SEED); np.random.seed(SEED)

SAVE_DIR = "/content/trained_models_xgb"
os.makedirs(SAVE_DIR, exist_ok=True)

print("✅ Ready.")

"""1) 데이터 생성 & 라벨링 (Δ + 요일 sin/cos → 16채널)

timesnet8_11(sin).py 흐름을 그대로 따름

7×7(원본) → Δ(7×7) 결합 → 요일 sin/cos(7×2) 추가 = (7×16)

스케일은 16채널에 대해 전체 fit (원본과 동일)
"""

FEATURES = ['총학습시간(분)','오전(분)','오후(분)','저녁(분)','심야(분)','반복횟수','일일달성률(%)']

# ▶ 우세 시간대 선택 시 확률(균등으로 하려면 [0.25,0.25,0.25,0.25])
_TIMESLOT_CHOICES = ['오전','오후','저녁','심야']
_TIMESLOT_PROBS   = [0.25, 0.25, 0.25, 0.25]  # 필요 시 조절

# ▶ 각 우세 시간대에 대응하는 Dirichlet 알파(해당 시간대에 큰 가중치)
_ALPHA_MAP = {
    '오전': np.array([8.0, 2.0, 2.0, 1.5]),  # [오전, 오후, 저녁, 심야]
    '오후': np.array([2.0, 8.0, 2.0, 1.5]),
    '저녁': np.array([2.0, 2.0, 8.0, 1.5]),
    '심야': np.array([1.5, 2.0, 2.0, 8.0]),
}

def _make_week_block(base_date, target_sincerity, zero_heavy=False):
    rows = []
    if target_sincerity == '고성실':
        total_mu, total_sigma = 180, 40
        ach_mu, ach_sigma = 85, 10
    elif target_sincerity == '중간성실':
        total_mu, total_sigma = 120, 40
        ach_mu, ach_sigma = 60, 15
    else:
        total_mu, total_sigma = 40, 30
        ach_mu, ach_sigma = 25, 20

    # ▶ 주차의 '우세 시간대'를 확률적으로 선택
    timeslot = np.random.choice(_TIMESLOT_CHOICES, p=_TIMESLOT_PROBS)
    alpha = _ALPHA_MAP[timeslot]

    for d in range(7):
        day_total = max(0, int(np.random.normal(total_mu, total_sigma)))
        if zero_heavy and np.random.rand() < 0.6:
            day_total = 0

        rep = 0 if day_total == 0 else int(max(0, np.random.poisson(lam=1.2)))
        daily_rate = int(np.clip(np.random.normal(ach_mu, ach_sigma), 0, 100))

        if day_total == 0:
            m=a=e=n=0
        else:
            # ▶ 4개 시간대를 한 번에 분배(정수, 합=day_total)
            probs = np.random.dirichlet(alpha)
            m, a, e, n = np.random.multinomial(day_total, probs)

            # 혹시나 전부 0으로 떨어지면(극단적인 경우) 한 분이라도 넣기
            if (m + a + e + n) == 0:
                n = day_total

        rows.append({
            '날짜': base_date + timedelta(days=d),
            '총학습시간(분)': day_total,
            '오전(분)': m,
            '오후(분)': a,
            '저녁(분)': e,
            '심야(분)': n,
            '반복횟수': rep,
            '일일달성률(%)': daily_rate,
        })
    return rows

def _sincerity_from_score(week_df):
    avg_total = week_df['총학습시간(분)'].mean()
    avg_rate  = week_df['일일달성률(%)'].mean()
    total_norm = min(avg_total / 180.0, 1.0)
    rate_norm  = avg_rate / 100.0
    score = 0.6*total_norm + 0.4*rate_norm
    if score >= 0.70: return '고성실'
    if score >= 0.40: return '중간성실'
    return '저성실'

def _timeslot_label(week_df):
    sums = {
        '오전': week_df['오전(분)'].sum(),
        '오후': week_df['오후(분)'].sum(),
        '저녁': week_df['저녁(분)'].sum(),
        '심야': week_df['심야(분)'].sum(),
    }
    return max(sums, key=sums.get)

def _repetition_label_by_rule(week_df):
    return '복습형' if float(week_df['반복횟수'].mean()) > 1.0 else '단기형'

def _augment_with_diff(X7):
    diff = np.diff(X7, axis=1, prepend=X7[:, :1, :])
    return np.concatenate([X7, diff], axis=2)  # (N,7,14)

def _append_weekday_sincos(X14):
    N,T,F = X14.shape
    dow = np.arange(T)  # 0..6
    sincos = np.stack([np.sin(2*np.pi*dow/7), np.cos(2*np.pi*dow/7)], axis=1)  # (7,2)
    sincos = np.tile(sincos, (N,1,1))  # (N,7,2)
    return np.concatenate([X14, sincos], axis=2)  # (N,7,16)

def build_dataset(n_each_class=300, zero_heavy_ratio=0.25):
    today = datetime.today().date()
    X_seqs, y_s, y_r, y_t = [], [], [], []

    classes = ['고성실','중간성실','저성실']
    total = n_each_class * len(classes)
    zero_flags = [True]*int(total*zero_heavy_ratio) + [False]*(total-int(total*zero_heavy_ratio))
    random.shuffle(zero_flags)

    idx_flag = 0
    for c in classes:
        for _ in range(n_each_class):
            base = today - timedelta(days=(total - len(X_seqs))*7)
            rows = _make_week_block(base, c, zero_heavy=zero_flags[idx_flag]); idx_flag += 1
            df = pd.DataFrame(rows)

            s = _sincerity_from_score(df)
            t = _timeslot_label(df)
            r = _repetition_label_by_rule(df)

            X_seqs.append(df[FEATURES].values)  # (7,7)
            y_s.append(s); y_r.append(r); y_t.append(t)

    X7  = np.array(X_seqs)          # (N,7,7)
    X14 = _augment_with_diff(X7)    # (N,7,14)
    X16 = _append_weekday_sincos(X14)

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X16.reshape(-1, X16.shape[2])).reshape(X16.shape)  # (N,7,16)

    le_s = LabelEncoder().fit(['저성실','중간성실','고성실'])
    le_r = LabelEncoder().fit(['단기형','복습형'])
    le_t = LabelEncoder().fit(['오전','오후','저녁','심야'])
    y_s_idx = le_s.transform(y_s)
    y_r_idx = le_r.transform(y_r)
    y_t_idx = le_t.transform(y_t)

    label_df = pd.DataFrame({'성실도': y_s, '복습형': y_r, '시간대': y_t})
    return X_scaled, y_s_idx, y_r_idx, y_t_idx, scaler, le_s, le_r, le_t, label_df

# ▶ 샘플 생성
X16, ys, yr, yt, scaler, le_s, le_r, le_t, label_df = build_dataset(
    n_each_class=250, zero_heavy_ratio=0.30
)

print("X shape (N,7,16):", X16.shape)
from collections import Counter
print("라벨 분포 성실도:", Counter(label_df['성실도']))
print("라벨 분포 복습형:", Counter(label_df['복습형']))
print("라벨 분포 시간대:", Counter(label_df['시간대']))

# ▶ 분포 시각화
fig, axes = plt.subplots(1, 3, figsize=(14,4))
for ax, col, title in zip(axes, ['성실도','복습형','시간대'], ['성실도', '복습형', '시간대']):
    order = (le_s.classes_ if col=='성실도' else
             (le_r.classes_ if col=='복습형' else le_t.classes_))
    counts = label_df[col].value_counts().reindex(order, fill_value=0)
    ax.bar(counts.index, counts.values)
    ax.set_title(f"{title} 분포")
    ax.grid(True, axis='y')
plt.tight_layout()
plt.show()

"""2) Train/Test 분리 + 특징 평탄화(112차원)

XGBoost는 2D 입력을 기대 → (7×16) → 112차원 벡터로 평탄화

층화는 성실도 기준(예시와 동일)
"""

# (N,7,16) → (N,112)
X = X16.reshape(len(X16), -1)

X_tr, X_te, ys_tr, ys_te, yr_tr, yr_te, yt_tr, yt_te = train_test_split(
    X, ys, yr, yt, test_size=0.2, random_state=SEED, stratify=ys
)
print("Train/Test:", X_tr.shape, X_te.shape)

# ▶ 클래스 가중치(불균형 완화): N / (K * count(c))
def make_sample_weights(y, n_classes):
    counts = np.bincount(y, minlength=n_classes)
    weights_per_class = (len(y) / (n_classes * np.maximum(counts, 1))).astype(np.float32)
    return weights_per_class[y], counts, weights_per_class

w_s_tr, cnt_s, wpc_s = make_sample_weights(ys_tr, len(le_s.classes_))
w_r_tr, cnt_r, wpc_r = make_sample_weights(yr_tr, len(le_r.classes_))
w_t_tr, cnt_t, wpc_t = make_sample_weights(yt_tr, len(le_t.classes_))

print("학습분포 성실도:", dict(zip(range(len(cnt_s)), cnt_s)))
print("학습분포 복습형:", dict(zip(range(len(cnt_r)), cnt_r)))
print("학습분포 시간대:", dict(zip(range(len(cnt_t)), cnt_t)))

"""3) XGBoost 3-헤드 학습 (성실도/복습형/시간대)

각 헤드별로 별도 분류기 학습

early stopping + eval_set 기록 → 학습 과정 출력/그래프 가능
"""

# === 블럭 3: xgboost.train 사용 (구버전 100% 호환) + early_stopping ===
import xgboost as xgb
import numpy as np
import matplotlib.pyplot as plt

def _train_with_train_api(X_tr, y_tr, X_te, y_te, n_classes, sample_weight=None, name="head"):
    """
    - DMatrix + xgb.train 사용 (구버전에서도 early_stopping_rounds 지원)
    - objective=multi:softprob, eval_metric=mlogloss
    - 반환: (booster, best_ntree_limit)
    """
    # DMatrix 구성 (가중치가 있으면 적용)
    dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=sample_weight)
    dvalid = xgb.DMatrix(X_te, label=y_te)

    params = {
        "objective": "multi:softprob",
        "num_class": n_classes,
        "eval_metric": "mlogloss",
        "max_depth": 6,
        "eta": 0.05,               # learning_rate
        "subsample": 0.9,
        "colsample_bytree": 0.9,
        "tree_method": "hist",
        "seed": SEED,
    }

    evals_result = {}
    booster = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=1200,
        evals=[(dvalid, "valid")],
        early_stopping_rounds=80,
        evals_result=evals_result,
        verbose_eval=False,
    )

    # best iteration / ntree_limit (버전별 호환)
    best_iter = getattr(booster, "best_iteration", None)
    best_ntree_limit = getattr(booster, "best_ntree_limit", None)
    # 구버전엔 best_iteration이 없고 best_ntree_limit만 있는 경우가 있음
    if best_ntree_limit is None and best_iter is not None:
        best_ntree_limit = best_iter + 1  # 트리 개수는 iteration+1로 보는게 일반적

    # best_score 추출
    best_score = None
    try:
        # evals_result 구조: {"valid": {"mlogloss": [ ... ]}}
        valid_hist = evals_result.get("valid", {}).get("mlogloss", [])
        if valid_hist:
            best_score = min(valid_hist)
    except Exception:
        pass

    print(f"✅ {name} | best_iter: {best_iter} | best_ntree_limit: {best_ntree_limit} | best_score(val mlogloss): {best_score}")
    return booster, best_ntree_limit, evals_result

# ▶ 3-헤드 학습 (성실도/복습형/시간대)
clf_s, ntree_s, res_s = _train_with_train_api(X_tr, ys_tr, X_te, ys_te, len(le_s.classes_), w_s_tr, name="성실도")
clf_r, ntree_r, res_r = _train_with_train_api(X_tr, yr_tr, X_te, yr_te, len(le_r.classes_), w_r_tr, name="복습형")
clf_t, ntree_t, res_t = _train_with_train_api(X_tr, yt_tr, X_te, yt_te, len(le_t.classes_), w_t_tr, name="시간대")

# === 학습 곡선(mlogloss) 시각화 ===
def plot_learning_curve_from_result(evals_result, title):
    vals = evals_result.get("valid", {}).get("mlogloss", None)
    if not vals:
        print(f"[WARN] {title}: validation mlogloss 기록이 없습니다.")
        return
    plt.figure()
    plt.plot(vals)
    plt.title(f"{title} mlogloss (val)")
    plt.xlabel("iteration")
    plt.ylabel("mlogloss")
    plt.grid(True)
    plt.show()

plot_learning_curve_from_result(res_s, "성실도")
plot_learning_curve_from_result(res_r, "복습형")
plot_learning_curve_from_result(res_t, "시간대")

# === (옵션) 예측 보조 함수 ===
def _predict_classes(booster, X, num_class, ntree_limit=None):
    """
    booster.predict로 확률 얻고 argmax → 라벨 인덱스 반환
    """
    dm = xgb.DMatrix(X)
    proba = booster.predict(dm, ntree_limit=ntree_limit)  # shape: (N, num_class)
    # 일부 구버전에선 2D가 아닌 1D로 나오는 경우가 거의 없지만, 안전장치:
    proba = np.asarray(proba).reshape(-1, num_class)
    return proba.argmax(axis=1)

"""4) 평가: 정확도/F1, 리포트, 혼동행렬, 중요도(상위 20)"""

# ============================
# 🔵 블럭 4 — 평가 & 시각화 (버전 호환)
# ============================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import xgboost as xgb

# (구)xgboost에서 predict가 DMatrix만 받는 경우 대비
def _predict_compat(model, X):
    """
    - 최신 버전: model.predict(np.ndarray) 정상 동작
    - 구버전: TypeError 발생 → DMatrix로 변환해서 재시도
    - 반환이 확률(2D)인 경우 argmax로 라벨 변환
    """
    try:
        y_pred = model.predict(X)
    except TypeError:
        # 구버전 호환: DMatrix로 감싸서 예측
        dm = xgb.DMatrix(X)
        y_pred = model.predict(dm)

    # multi:softprob에서 booster.predict를 쓴 경우 2D 확률일 수 있음
    if isinstance(y_pred, np.ndarray) and y_pred.ndim == 2:
        y_pred = np.argmax(y_pred, axis=1)
    return y_pred

def eval_head(model, X_te, y_te, le, title):
    y_pred = _predict_compat(model, X_te)

    acc = accuracy_score(y_te, y_pred)
    f1m = f1_score(y_te, y_pred, average="macro")
    print(f"[{title}] acc={acc:.4f}, f1(macro)={f1m:.4f}\n")
    print(classification_report(y_te, y_pred, target_names=le.classes_))

    # 혼동행렬
    cm = confusion_matrix(y_te, y_pred, labels=range(len(le.classes_)))
    plt.figure(figsize=(5.2, 4.6))
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title(f"{title} Confusion Matrix")
    plt.xticks(range(len(le.classes_)), le.classes_, rotation=45, ha='right')
    plt.yticks(range(len(le.classes_)), le.classes_)
    plt.colorbar()
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i, j], ha="center", va="center", fontsize=10)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()

    return y_pred

# ── 3개 헤드 평가 ─────────────────────────────────────────────
ps = eval_head(clf_s, X_te, ys_te, le_s, "성실도")
pr = eval_head(clf_r, X_te, yr_te, le_r, "복습형")
pt = eval_head(clf_t, X_te, yt_te, le_t, "시간대")

# ── 중요도(성실도 헤드, 상위 20개) ───────────────────────────
def plot_importance_topk(model, topk=20, title="Feature Importance (성실도 상위 20)"):
    """
    우선 sklearn 래퍼의 feature_importances_ 사용.
    불가하면 booster의 get_score(importance_type='gain')를 사용(키 f0,f1,...).
    """
    imp = None
    # 1) sklearn 래퍼 경로
    if hasattr(model, "feature_importances_"):
        imp = model.feature_importances_
        idx = np.argsort(imp)[::-1][:topk]
        vals = imp[idx]
        labels = [f"f{int(i)}" for i in idx]
    else:
        # 2) booster 경로
        try:
            booster = model.get_booster()
            score_dict = booster.get_score(importance_type='gain')  # {'f0':gain, ...}
            if not score_dict:
                print("⚠️ 중요도 정보를 가져올 수 없습니다.")
                return
            # dict → 정렬
            items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)[:topk]
            labels = [k for k, _ in items]
            vals = [v for _, v in items]
        except Exception:
            print("⚠️ 중요도 정보를 가져올 수 없습니다.")
            return

    plt.figure(figsize=(9, 4.6))
    plt.bar(range(len(vals)), vals)
    plt.xticks(range(len(vals)), labels, rotation=45, ha='right')
    plt.title(title)
    plt.ylabel("gain")
    plt.grid(True, axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

plot_importance_topk(clf_s, topk=20, title="Feature Importance (성실도 상위 20)")


# ====== Booster 중요도 시각화 유틸 ======

import numpy as np
import matplotlib.pyplot as plt

# (선택) 한글 폰트 경고 제거 – Colab 기본 폰트 대체
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'  # 경고만 줄이고, 레이블은 영문/기호 위주로 표시
# 만약 진짜 한글 폰트가 필요하면:
# !apt-get -y install fonts-nanum
# import matplotlib.font_manager as fm
# fm._rebuild()
# plt.rcParams['font.family'] = 'NanumGothic'

# 1) 평탄화 인덱스 → 사람이 읽는 이름 만들기
base_feats  = ['총','오전','오후','저녁','심야','반복','달성']           # 7개
delta_feats = ['Δ총','Δ오전','Δ오후','Δ저녁','Δ심야','Δ반복','Δ달성']    # 7개
day_feats   = base_feats + delta_feats + ['sin','cos']                  # 16개

day_names = ['D0(월)','D1(화)','D2(수)','D3(목)','D4(금)','D5(토)','D6(일)']

def flat_index_to_name(fid: int) -> str:
    d = fid // 16   # day index (0..6)
    f = fid % 16    # feature within day (0..15)
    return f"{day_names[d]}:{day_feats[f]}"

def booster_importance_bar(booster, top_k=20, title="Feature Importance (gain)"):
    # 2) Booster의 get_score로 중요도 가져오기 (gain/weight/cover 등 선택 가능)
    #    키는 'f0','f1',... 형태거나, feature_names를 지정했으면 그 이름으로 옴.
    score_dict = booster.get_score(importance_type='gain')

    if not score_dict:
        print("⚠️ 모델에서 중요도가 비었습니다. (트리가 거의 안생겼거나, Booster가 맞는지 확인 필요)")
        return

    # 3) 키를 정수 인덱스로 파싱 (예: 'f37' -> 37)
    idxs, gains = [], []
    for k, v in score_dict.items():
        if k.startswith('f') and k[1:].isdigit():
            idxs.append(int(k[1:]))
            gains.append(float(v))
        else:
            # 혹시 이름이 문자열로 들어왔으면 그대로 보이게만 처리
            # (이번 파이프라인에선 'fN' 형태일 것)
            pass

    if not idxs:
        print("⚠️ 중요도 키가 'f0' 형식이 아닙니다. feature_names를 지정했는지 확인하세요.")
        # 그래도 그려보기
        items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)[:top_k]
        labels = [k for k, _ in items]
        vals   = [v for _, v in items]
        plt.figure(figsize=(9,4.8))
        plt.bar(range(len(vals)), vals)
        plt.xticks(range(len(vals)), labels, rotation=60, ha='right')
        plt.title(title)
        plt.grid(True, axis='y')
        plt.tight_layout(); plt.show()
        return

    idxs = np.array(idxs); gains = np.array(gains)

    # 4) 상위 top_k 고르기
    order = np.argsort(gains)[::-1]
    top = order[:top_k]
    top_idxs  = idxs[top]
    top_gains = gains[top]
    top_names = [flat_index_to_name(i) for i in top_idxs]

    # 5) 그리기
    plt.figure(figsize=(10,5))
    plt.bar(range(len(top_gains)), top_gains)
    plt.xticks(range(len(top_gains)), top_names, rotation=60, ha='right')
    plt.title(title)
    plt.ylabel('gain')
    plt.grid(True, axis='y')
    plt.tight_layout()
    plt.show()

# === 호출예: 각 헤드의 Booster를 넘겨주세요 ===
#  - 방법1(xgb.train)로 학습했다면 clf_* 자체가 Booster 입니다.
booster_importance_bar(clf_s, top_k=20, title="성실도 중요도 (gain)")
booster_importance_bar(clf_r, top_k=20, title="복습형 중요도 (gain)")
booster_importance_bar(clf_t, top_k=20, title="시간대 중요도 (gain)")

"""5) 저장 + 추론 함수 (옵션)"""

import xgboost as xgb
import numpy as np

def _predict_classes_booster(booster, X2d, num_class,
                             end_iter=None,  # 새 API용 (iteration_range)
                             ntree_limit=None):  # 구 API용
    """
    - XGBoost 버전별 호환:
      1) iteration_range=(0, end_iter) 시도
      2) 실패하면 ntree_limit=end_iter 시도
      3) 그래도 실패하면 ntree_limit 파라미터 사용 or 제한 없이 예측
    """
    dm = xgb.DMatrix(X2d)

    # 1) 새 API 먼저 시도
    if end_iter is not None:
        try:
            proba = booster.predict(dm, iteration_range=(0, int(end_iter)))
        except TypeError:
            # 2) 구 API 대체 시도
            try:
                proba = booster.predict(dm, ntree_limit=int(end_iter))
            except TypeError:
                proba = booster.predict(dm)
    else:
        # end_iter가 없으면 ntree_limit로 시도
        if ntree_limit is not None:
            try:
                proba = booster.predict(dm, ntree_limit=int(ntree_limit))
            except TypeError:
                try:
                    proba = booster.predict(dm, iteration_range=(0, int(ntree_limit)))
                except TypeError:
                    proba = booster.predict(dm)
        else:
            proba = booster.predict(dm)

    proba = np.asarray(proba).reshape(-1, num_class)
    return proba.argmax(axis=1)

def predict_user_type_xgb(week_7x7,
                          booster_s=clf_s, booster_r=clf_r, booster_t=clf_t,
                          scaler16=scaler, le_s=le_s, le_r=le_r, le_t=le_t,
                          meta=meta if 'meta' in globals() else None):
    """
    week_7x7: 7일 x 7특징 [총, 오전, 오후, 저녁, 심야, 반복횟수, 일일달성률]
    - Δ + 요일 sin/cos → 스케일 → (1,112)
    - Booster 예측: 버전에 맞춰 iteration_range 또는 ntree_limit 자동 사용
    """
    arr = np.array(week_7x7, dtype=np.float32).reshape(7,7)

    # Δ + sin/cos → (7,16)
    diff  = np.diff(arr, axis=0, prepend=arr[:1,:])
    arr14 = np.concatenate([arr, diff], axis=1)
    dow = np.arange(7)
    sincos = np.stack([np.sin(2*np.pi*dow/7), np.cos(2*np.pi*dow/7)], axis=1)
    arr16 = np.concatenate([arr14, sincos], axis=1)

    # 스케일 & 평탄화
    flat = scaler16.transform(arr16.reshape(-1, arr16.shape[1])).reshape(1, -1)  # (1,112)

    # 메타에서 best iteration/ntree_limit 읽기
    end_s = end_r = end_t = None
    nls = nlr = nlt = None
    if meta is not None:
        # 새 형식: meta["best_iter"][...], 구 형식: meta["ntree_limit"][...]
        bi = meta.get("best_iter", {})
        nl = meta.get("ntree_limit", {})
        end_s = bi.get("sincerity", None) or nl.get("sincerity", None)
        end_r = bi.get("repetition", None) or nl.get("repetition", None)
        end_t = bi.get("timeslot",  None) or nl.get("timeslot",  None)
        # 호환 위해 ntree_limit 값도 들고 있음
        nls = nl.get("sincerity", None); nlr = nl.get("repetition", None); nlt = nl.get("timeslot", None)

    s_idx = int(_predict_classes_booster(booster_s, flat, len(le_s.classes_),
                                         end_iter=end_s, ntree_limit=nls)[0])
    r_idx = int(_predict_classes_booster(booster_r, flat, len(le_r.classes_),
                                         end_iter=end_r, ntree_limit=nlr)[0])
    t_idx = int(_predict_classes_booster(booster_t, flat, len(le_t.classes_),
                                         end_iter=end_t, ntree_limit=nlt)[0])

    return {
        "성실도": le_s.inverse_transform([s_idx])[0],
        "반복형": le_r.inverse_transform([r_idx])[0],
        "시간대": le_t.inverse_transform([t_idx])[0],
    }

# ===== Block 6. 로드 & 예측 테스트 =====
import os, json, pickle, joblib
import numpy as np
import xgboost as xgb

# ▶ 훈련 때 저장한 경로와 동일해야 합니다.
SAVE_DIR = "/content/trained_models_xgb"  # 필요시 변경

# -----------------------------
# 1) 아티팩트 로더
# -----------------------------
def load_all(save_dir: str):
    """
    반환:
      - clf_s, clf_r, clf_t: XGBClassifier 또는 xgb.Booster 객체(둘 중 무엇이든 OK)
      - scaler: MinMaxScaler (7→14→16 전처리 후 16차원에 대해 학습된 스케일러)
      - enc: {"sincerity": LabelEncoder, "repetition": LabelEncoder, "timeslot": LabelEncoder}
      - meta: {"ntree_limit": {...}} 또는 {}
    """
    # 스케일러
    scaler = joblib.load(os.path.join(save_dir, "scaler16.joblib"))

    # 라벨 인코더
    with open(os.path.join(save_dir, "label_encoders.pkl"), "rb") as f:
        enc = pickle.load(f)

    # 메타 (없어도 동작)
    meta_path = os.path.join(save_dir, "meta.json")
    meta = {}
    if os.path.exists(meta_path):
        with open(meta_path, "r") as f:
            meta = json.load(f)

    # 분류기(XGBClassifier) 먼저 시도, 없으면 Booster 로드
    def _load_head(prefix):
        cls_path = os.path.join(save_dir, f"{prefix}.joblib")
        bst_path = os.path.join(save_dir, f"{prefix}.booster")
        if os.path.exists(cls_path):
            return joblib.load(cls_path)
        elif os.path.exists(bst_path):
            bst = xgb.Booster()
            bst.load_model(bst_path)
            return bst
        else:
            raise FileNotFoundError(f"{prefix} 모델 파일을 찾을 수 없습니다: {cls_path} / {bst_path}")

    clf_s = _load_head("xgb_sincerity")
    clf_r = _load_head("xgb_repetition")
    clf_t = _load_head("xgb_timeslot")

    return clf_s, clf_r, clf_t, scaler, enc, meta

# -----------------------------
# 2) 7x7 → 7x16 전처리 (훈련 때와 동일)
# -----------------------------
def _to_16_features(week_7x7: np.ndarray) -> np.ndarray:
    """
    입력: (7,7) [총,오전,오후,저녁,심야,반복,달성률]
    출력: (7,16) = 원본7 + Δ7 + 요일 sin/cos(2)
    """
    arr = np.array(week_7x7, dtype=np.float32).reshape(7, 7)
    diff = np.diff(arr, axis=0, prepend=arr[:1, :])           # (7,7)
    arr14 = np.concatenate([arr, diff], axis=1)               # (7,14)
    dow = np.arange(7)
    sincos = np.stack([np.sin(2*np.pi*dow/7), np.cos(2*np.pi*dow/7)], axis=1)  # (7,2)
    arr16 = np.concatenate([arr14, sincos], axis=1)           # (7,16)
    return arr16

# -----------------------------
# 3) 분류기/부스터 호환 예측 헬퍼
# -----------------------------
def _predict_head(model, flat_112: np.ndarray, n_classes: int, ntree_limit: int | None = None):
    """
    model: XGBClassifier 또는 xgb.Booster
    flat_112: (1, 112)
    n_classes: 클래스 수
    ntree_limit: Early Stopping 최적 트리 수 (없으면 전체 사용)

    반환: argmax(class) 정수 id
    """
    if isinstance(model, xgb.Booster):
        dtest = xgb.DMatrix(flat_112)
        # 최신/구버전 호환: ntree_limit 대신 iteration_range 사용
        # ntree_limit가 없으면 전체 트리 사용
        if ntree_limit and ntree_limit > 0:
            # iteration_range는 [start, end) 이므로 end=ntree_limit 로 주면 ntree_limit개 사용
            prob = model.predict(dtest, iteration_range=(0, ntree_limit))
        else:
            prob = model.predict(dtest)
        # multi:softprob → (1, n_classes)
        cls_id = int(np.argmax(prob.reshape(-1, n_classes), axis=1)[0])
        return cls_id
    else:
        # XGBClassifier
        prob = model.predict_proba(flat_112)
        cls_id = int(np.argmax(prob.reshape(-1, n_classes), axis=1)[0])
        return cls_id

# -----------------------------
# 4) 일괄 예측 함수 (사전/사후 인코딩 포함)
# -----------------------------
def predict_user_type_xgb(
    week_7x7,
    booster_s=None, booster_r=None, booster_t=None,
    scaler16=None, le_s=None, le_r=None, le_t=None,
    ntree_s: int | None = None, ntree_r: int | None = None, ntree_t: int | None = None,
):
    """
    기본적으로 SAVE_DIR에서 로드된 아티팩트를 사용.
    필요 시 인자로 모델/스케일러/인코더/ntree 정보를 직접 주입해도 OK.
    """
    # 로컬 아티팩트 자동 로드
    if any(v is None for v in [booster_s, booster_r, booster_t, scaler16, le_s, le_r, le_t]):
        booster_s, booster_r, booster_t, scaler16, enc, meta = load_all(SAVE_DIR)
        le_s, le_r, le_t = enc["sincerity"], enc["repetition"], enc["timeslot"]
        # 메타에서 최적 트리 수 있으면 반영
        nmeta = meta.get("ntree_limit", {}) if isinstance(meta, dict) else {}
        ntree_s = ntree_s or nmeta.get("sincerity", None)
        ntree_r = ntree_r or nmeta.get("repetition", None)
        ntree_t = ntree_t or nmeta.get("timeslot", None)

    # 7x7 → 7x16 → 스케일 → (1,112)
    arr16 = _to_16_features(week_7x7)
    flat = scaler16.transform(arr16.reshape(-1, arr16.shape[1])).reshape(1, -1)

    # 3헤드 예측
    sid = _predict_head(booster_s, flat, n_classes=len(le_s.classes_), ntree_limit=ntree_s)
    rid = _predict_head(booster_r, flat, n_classes=len(le_r.classes_), ntree_limit=ntree_r)
    tid = _predict_head(booster_t, flat, n_classes=len(le_t.classes_), ntree_limit=ntree_t)

    s_lab = le_s.inverse_transform([sid])[0]
    r_lab = le_r.inverse_transform([rid])[0]
    t_lab = le_t.inverse_transform([tid])[0]
    return {"성실도": s_lab, "반복형": r_lab, "시간대": t_lab}

# -----------------------------
# 5) 샘플 생성기(간단) & 테스트
# -----------------------------
def gen_sample_week(timeslot="오전", total_mu=130, total_sigma=40, ach_mu=70, ach_sigma=15, zero_day_p=0.15):
    """
    간단 샘플러: 지정한 주 집중 시간대에 비중을 더 줘서 7x7 주 데이터를 생성
    """
    rows = []
    for d in range(7):
        # 총 학습시간
        tot = max(0, int(np.random.normal(total_mu, total_sigma)))
        if np.random.rand() < zero_day_p:
            tot = 0
        # 달성률/반복
        rep = 0 if tot == 0 else int(max(0, np.random.poisson(lam=1.2)))
        ach = int(np.clip(np.random.normal(ach_mu, ach_sigma), 0, 100))

        if tot == 0:
            m=a=e=n=0
        else:
            if timeslot == "오전":
                m_ratio = np.random.uniform(0.55, 0.85)
                a_ratio = np.random.uniform(0.05, 0.25)
                e_ratio = np.random.uniform(0.05, 0.25)
            elif timeslot == "오후":
                a_ratio = np.random.uniform(0.55, 0.85)
                m_ratio = np.random.uniform(0.05, 0.25)
                e_ratio = np.random.uniform(0.05, 0.25)
            elif timeslot == "저녁":
                e_ratio = np.random.uniform(0.55, 0.85)
                m_ratio = np.random.uniform(0.05, 0.25)
                a_ratio = np.random.uniform(0.05, 0.25)
            else:  # 심야
                m_ratio = np.random.uniform(0.01, 0.15)
                a_ratio = np.random.uniform(0.01, 0.15)
                e_ratio = np.random.uniform(0.01, 0.15)
            s = m_ratio + a_ratio + e_ratio
            m = int(tot * (m_ratio / (s + 1e-6)))
            a = int(tot * (a_ratio / (s + 1e-6)))
            e = int(tot * (e_ratio / (s + 1e-6)))
            n = max(0, tot - m - a - e)

        rows.append([tot, m, a, e, n, rep, ach])
    return rows

# ── 저장물 로드(한 번만)
clf_s, clf_r, clf_t, scaler, enc, meta = load_all(SAVE_DIR)
le_s, le_r, le_t = enc["sincerity"], enc["repetition"], enc["timeslot"]
ntree = meta.get("ntree_limit", {}) if isinstance(meta, dict) else {}
print("Loaded. ntree_limit:", ntree)

# ── ① 문서 예제와 유사한 수동 샘플
# example = [
#     [300, 50, 200, 40, 10, 2, 100],
#     [300, 20, 240, 30, 10, 1, 60],
#     [180, 60, 60, 50, 10, 2, 100],
#     [0,   0,  0,  0,  0, 0,  0],
#     [300,  10, 250, 30, 10, 2, 100],
#     [140, 40, 50, 40, 10, 1, 100],
#     [130, 20, 70, 30, 10, 2, 100],
# ]
example = [
    [0, 0, 0, 0, 0, 2, 0],
    [0, 0, 0, 0, 0, 1, 0],
    [180, 60, 60, 50, 10, 2, 100],
    [0,   0,  0,  0,  0, 0,  0],
    [0,  0, 0, 0, 0, 2, 0],
    [0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 2, 0],
]
print("샘플 예측(수동):",
      predict_user_type_xgb(
          example,
          booster_s=clf_s, booster_r=clf_r, booster_t=clf_t,
          scaler16=scaler, le_s=le_s, le_r=le_r, le_t=le_t,
          ntree_s=ntree.get("sincerity"), ntree_r=ntree.get("repetition"), ntree_t=ntree.get("timeslot"))
)

# ── ② 랜덤 샘플(시간대별 1개씩)
for ts in ["오전", "오후", "저녁", "심야"]:
    sample = gen_sample_week(timeslot=ts)
    pred = predict_user_type_xgb(
        sample,
        booster_s=clf_s, booster_r=clf_r, booster_t=clf_t,
        scaler16=scaler, le_s=le_s, le_r=le_r, le_t=le_t,
        ntree_s=ntree.get("sincerity"), ntree_r=ntree.get("repetition"), ntree_t=ntree.get("timeslot"))
    print(f"[랜덤 {ts}] → {pred}")