# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/194CF-sEAjeXyqliRW6S1Sl83A5nlGDy-

0) í™˜ê²½ ì¤€ë¹„
"""

!pip -q install xgboost==2.0.3

import os, math, random, pickle
import numpy as np
import pandas as pd
from collections import Counter
from datetime import datetime, timedelta

import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix,
    f1_score, accuracy_score
)

import xgboost as xgb

SEED = 42
random.seed(SEED); np.random.seed(SEED)

SAVE_DIR = "/content/trained_models_xgb"
os.makedirs(SAVE_DIR, exist_ok=True)

print("âœ… Ready.")

"""1) ë°ì´í„° ìƒì„± & ë¼ë²¨ë§ (Î” + ìš”ì¼ sin/cos â†’ 16ì±„ë„)

timesnet8_11(sin).py íë¦„ì„ ê·¸ëŒ€ë¡œ ë”°ë¦„

7Ã—7(ì›ë³¸) â†’ Î”(7Ã—7) ê²°í•© â†’ ìš”ì¼ sin/cos(7Ã—2) ì¶”ê°€ = (7Ã—16)

ìŠ¤ì¼€ì¼ì€ 16ì±„ë„ì— ëŒ€í•´ ì „ì²´ fit (ì›ë³¸ê³¼ ë™ì¼)
"""

FEATURES = ['ì´í•™ìŠµì‹œê°„(ë¶„)','ì˜¤ì „(ë¶„)','ì˜¤í›„(ë¶„)','ì €ë…(ë¶„)','ì‹¬ì•¼(ë¶„)','ë°˜ë³µíšŸìˆ˜','ì¼ì¼ë‹¬ì„±ë¥ (%)']

# â–¶ ìš°ì„¸ ì‹œê°„ëŒ€ ì„ íƒ ì‹œ í™•ë¥ (ê· ë“±ìœ¼ë¡œ í•˜ë ¤ë©´ [0.25,0.25,0.25,0.25])
_TIMESLOT_CHOICES = ['ì˜¤ì „','ì˜¤í›„','ì €ë…','ì‹¬ì•¼']
_TIMESLOT_PROBS   = [0.25, 0.25, 0.25, 0.25]  # í•„ìš” ì‹œ ì¡°ì ˆ

# â–¶ ê° ìš°ì„¸ ì‹œê°„ëŒ€ì— ëŒ€ì‘í•˜ëŠ” Dirichlet ì•ŒíŒŒ(í•´ë‹¹ ì‹œê°„ëŒ€ì— í° ê°€ì¤‘ì¹˜)
_ALPHA_MAP = {
    'ì˜¤ì „': np.array([8.0, 2.0, 2.0, 1.5]),  # [ì˜¤ì „, ì˜¤í›„, ì €ë…, ì‹¬ì•¼]
    'ì˜¤í›„': np.array([2.0, 8.0, 2.0, 1.5]),
    'ì €ë…': np.array([2.0, 2.0, 8.0, 1.5]),
    'ì‹¬ì•¼': np.array([1.5, 2.0, 2.0, 8.0]),
}

def _make_week_block(base_date, target_sincerity, zero_heavy=False):
    rows = []
    if target_sincerity == 'ê³ ì„±ì‹¤':
        total_mu, total_sigma = 180, 40
        ach_mu, ach_sigma = 85, 10
    elif target_sincerity == 'ì¤‘ê°„ì„±ì‹¤':
        total_mu, total_sigma = 120, 40
        ach_mu, ach_sigma = 60, 15
    else:
        total_mu, total_sigma = 40, 30
        ach_mu, ach_sigma = 25, 20

    # â–¶ ì£¼ì°¨ì˜ 'ìš°ì„¸ ì‹œê°„ëŒ€'ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì„ íƒ
    timeslot = np.random.choice(_TIMESLOT_CHOICES, p=_TIMESLOT_PROBS)
    alpha = _ALPHA_MAP[timeslot]

    for d in range(7):
        day_total = max(0, int(np.random.normal(total_mu, total_sigma)))
        if zero_heavy and np.random.rand() < 0.6:
            day_total = 0

        rep = 0 if day_total == 0 else int(max(0, np.random.poisson(lam=1.2)))
        daily_rate = int(np.clip(np.random.normal(ach_mu, ach_sigma), 0, 100))

        if day_total == 0:
            m=a=e=n=0
        else:
            # â–¶ 4ê°œ ì‹œê°„ëŒ€ë¥¼ í•œ ë²ˆì— ë¶„ë°°(ì •ìˆ˜, í•©=day_total)
            probs = np.random.dirichlet(alpha)
            m, a, e, n = np.random.multinomial(day_total, probs)

            # í˜¹ì‹œë‚˜ ì „ë¶€ 0ìœ¼ë¡œ ë–¨ì–´ì§€ë©´(ê·¹ë‹¨ì ì¸ ê²½ìš°) í•œ ë¶„ì´ë¼ë„ ë„£ê¸°
            if (m + a + e + n) == 0:
                n = day_total

        rows.append({
            'ë‚ ì§œ': base_date + timedelta(days=d),
            'ì´í•™ìŠµì‹œê°„(ë¶„)': day_total,
            'ì˜¤ì „(ë¶„)': m,
            'ì˜¤í›„(ë¶„)': a,
            'ì €ë…(ë¶„)': e,
            'ì‹¬ì•¼(ë¶„)': n,
            'ë°˜ë³µíšŸìˆ˜': rep,
            'ì¼ì¼ë‹¬ì„±ë¥ (%)': daily_rate,
        })
    return rows

def _sincerity_from_score(week_df):
    avg_total = week_df['ì´í•™ìŠµì‹œê°„(ë¶„)'].mean()
    avg_rate  = week_df['ì¼ì¼ë‹¬ì„±ë¥ (%)'].mean()
    total_norm = min(avg_total / 180.0, 1.0)
    rate_norm  = avg_rate / 100.0
    score = 0.6*total_norm + 0.4*rate_norm
    if score >= 0.70: return 'ê³ ì„±ì‹¤'
    if score >= 0.40: return 'ì¤‘ê°„ì„±ì‹¤'
    return 'ì €ì„±ì‹¤'

def _timeslot_label(week_df):
    sums = {
        'ì˜¤ì „': week_df['ì˜¤ì „(ë¶„)'].sum(),
        'ì˜¤í›„': week_df['ì˜¤í›„(ë¶„)'].sum(),
        'ì €ë…': week_df['ì €ë…(ë¶„)'].sum(),
        'ì‹¬ì•¼': week_df['ì‹¬ì•¼(ë¶„)'].sum(),
    }
    return max(sums, key=sums.get)

def _repetition_label_by_rule(week_df):
    return 'ë³µìŠµí˜•' if float(week_df['ë°˜ë³µíšŸìˆ˜'].mean()) > 1.0 else 'ë‹¨ê¸°í˜•'

def _augment_with_diff(X7):
    diff = np.diff(X7, axis=1, prepend=X7[:, :1, :])
    return np.concatenate([X7, diff], axis=2)  # (N,7,14)

def _append_weekday_sincos(X14):
    N,T,F = X14.shape
    dow = np.arange(T)  # 0..6
    sincos = np.stack([np.sin(2*np.pi*dow/7), np.cos(2*np.pi*dow/7)], axis=1)  # (7,2)
    sincos = np.tile(sincos, (N,1,1))  # (N,7,2)
    return np.concatenate([X14, sincos], axis=2)  # (N,7,16)

def build_dataset(n_each_class=300, zero_heavy_ratio=0.25):
    today = datetime.today().date()
    X_seqs, y_s, y_r, y_t = [], [], [], []

    classes = ['ê³ ì„±ì‹¤','ì¤‘ê°„ì„±ì‹¤','ì €ì„±ì‹¤']
    total = n_each_class * len(classes)
    zero_flags = [True]*int(total*zero_heavy_ratio) + [False]*(total-int(total*zero_heavy_ratio))
    random.shuffle(zero_flags)

    idx_flag = 0
    for c in classes:
        for _ in range(n_each_class):
            base = today - timedelta(days=(total - len(X_seqs))*7)
            rows = _make_week_block(base, c, zero_heavy=zero_flags[idx_flag]); idx_flag += 1
            df = pd.DataFrame(rows)

            s = _sincerity_from_score(df)
            t = _timeslot_label(df)
            r = _repetition_label_by_rule(df)

            X_seqs.append(df[FEATURES].values)  # (7,7)
            y_s.append(s); y_r.append(r); y_t.append(t)

    X7  = np.array(X_seqs)          # (N,7,7)
    X14 = _augment_with_diff(X7)    # (N,7,14)
    X16 = _append_weekday_sincos(X14)

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X16.reshape(-1, X16.shape[2])).reshape(X16.shape)  # (N,7,16)

    le_s = LabelEncoder().fit(['ì €ì„±ì‹¤','ì¤‘ê°„ì„±ì‹¤','ê³ ì„±ì‹¤'])
    le_r = LabelEncoder().fit(['ë‹¨ê¸°í˜•','ë³µìŠµí˜•'])
    le_t = LabelEncoder().fit(['ì˜¤ì „','ì˜¤í›„','ì €ë…','ì‹¬ì•¼'])
    y_s_idx = le_s.transform(y_s)
    y_r_idx = le_r.transform(y_r)
    y_t_idx = le_t.transform(y_t)

    label_df = pd.DataFrame({'ì„±ì‹¤ë„': y_s, 'ë³µìŠµí˜•': y_r, 'ì‹œê°„ëŒ€': y_t})
    return X_scaled, y_s_idx, y_r_idx, y_t_idx, scaler, le_s, le_r, le_t, label_df

# â–¶ ìƒ˜í”Œ ìƒì„±
X16, ys, yr, yt, scaler, le_s, le_r, le_t, label_df = build_dataset(
    n_each_class=250, zero_heavy_ratio=0.30
)

print("X shape (N,7,16):", X16.shape)
from collections import Counter
print("ë¼ë²¨ ë¶„í¬ ì„±ì‹¤ë„:", Counter(label_df['ì„±ì‹¤ë„']))
print("ë¼ë²¨ ë¶„í¬ ë³µìŠµí˜•:", Counter(label_df['ë³µìŠµí˜•']))
print("ë¼ë²¨ ë¶„í¬ ì‹œê°„ëŒ€:", Counter(label_df['ì‹œê°„ëŒ€']))

# â–¶ ë¶„í¬ ì‹œê°í™”
fig, axes = plt.subplots(1, 3, figsize=(14,4))
for ax, col, title in zip(axes, ['ì„±ì‹¤ë„','ë³µìŠµí˜•','ì‹œê°„ëŒ€'], ['ì„±ì‹¤ë„', 'ë³µìŠµí˜•', 'ì‹œê°„ëŒ€']):
    order = (le_s.classes_ if col=='ì„±ì‹¤ë„' else
             (le_r.classes_ if col=='ë³µìŠµí˜•' else le_t.classes_))
    counts = label_df[col].value_counts().reindex(order, fill_value=0)
    ax.bar(counts.index, counts.values)
    ax.set_title(f"{title} ë¶„í¬")
    ax.grid(True, axis='y')
plt.tight_layout()
plt.show()

"""2) Train/Test ë¶„ë¦¬ + íŠ¹ì§• í‰íƒ„í™”(112ì°¨ì›)

XGBoostëŠ” 2D ì…ë ¥ì„ ê¸°ëŒ€ â†’ (7Ã—16) â†’ 112ì°¨ì› ë²¡í„°ë¡œ í‰íƒ„í™”

ì¸µí™”ëŠ” ì„±ì‹¤ë„ ê¸°ì¤€(ì˜ˆì‹œì™€ ë™ì¼)
"""

# (N,7,16) â†’ (N,112)
X = X16.reshape(len(X16), -1)

X_tr, X_te, ys_tr, ys_te, yr_tr, yr_te, yt_tr, yt_te = train_test_split(
    X, ys, yr, yt, test_size=0.2, random_state=SEED, stratify=ys
)
print("Train/Test:", X_tr.shape, X_te.shape)

# â–¶ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜(ë¶ˆê· í˜• ì™„í™”): N / (K * count(c))
def make_sample_weights(y, n_classes):
    counts = np.bincount(y, minlength=n_classes)
    weights_per_class = (len(y) / (n_classes * np.maximum(counts, 1))).astype(np.float32)
    return weights_per_class[y], counts, weights_per_class

w_s_tr, cnt_s, wpc_s = make_sample_weights(ys_tr, len(le_s.classes_))
w_r_tr, cnt_r, wpc_r = make_sample_weights(yr_tr, len(le_r.classes_))
w_t_tr, cnt_t, wpc_t = make_sample_weights(yt_tr, len(le_t.classes_))

print("í•™ìŠµë¶„í¬ ì„±ì‹¤ë„:", dict(zip(range(len(cnt_s)), cnt_s)))
print("í•™ìŠµë¶„í¬ ë³µìŠµí˜•:", dict(zip(range(len(cnt_r)), cnt_r)))
print("í•™ìŠµë¶„í¬ ì‹œê°„ëŒ€:", dict(zip(range(len(cnt_t)), cnt_t)))

"""3) XGBoost 3-í—¤ë“œ í•™ìŠµ (ì„±ì‹¤ë„/ë³µìŠµí˜•/ì‹œê°„ëŒ€)

ê° í—¤ë“œë³„ë¡œ ë³„ë„ ë¶„ë¥˜ê¸° í•™ìŠµ

early stopping + eval_set ê¸°ë¡ â†’ í•™ìŠµ ê³¼ì • ì¶œë ¥/ê·¸ë˜í”„ ê°€ëŠ¥
"""

# === ë¸”ëŸ­ 3: xgboost.train ì‚¬ìš© (êµ¬ë²„ì „ 100% í˜¸í™˜) + early_stopping ===
import xgboost as xgb
import numpy as np
import matplotlib.pyplot as plt

def _train_with_train_api(X_tr, y_tr, X_te, y_te, n_classes, sample_weight=None, name="head"):
    """
    - DMatrix + xgb.train ì‚¬ìš© (êµ¬ë²„ì „ì—ì„œë„ early_stopping_rounds ì§€ì›)
    - objective=multi:softprob, eval_metric=mlogloss
    - ë°˜í™˜: (booster, best_ntree_limit)
    """
    # DMatrix êµ¬ì„± (ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´ ì ìš©)
    dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=sample_weight)
    dvalid = xgb.DMatrix(X_te, label=y_te)

    params = {
        "objective": "multi:softprob",
        "num_class": n_classes,
        "eval_metric": "mlogloss",
        "max_depth": 6,
        "eta": 0.05,               # learning_rate
        "subsample": 0.9,
        "colsample_bytree": 0.9,
        "tree_method": "hist",
        "seed": SEED,
    }

    evals_result = {}
    booster = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=1200,
        evals=[(dvalid, "valid")],
        early_stopping_rounds=80,
        evals_result=evals_result,
        verbose_eval=False,
    )

    # best iteration / ntree_limit (ë²„ì „ë³„ í˜¸í™˜)
    best_iter = getattr(booster, "best_iteration", None)
    best_ntree_limit = getattr(booster, "best_ntree_limit", None)
    # êµ¬ë²„ì „ì—” best_iterationì´ ì—†ê³  best_ntree_limitë§Œ ìˆëŠ” ê²½ìš°ê°€ ìˆìŒ
    if best_ntree_limit is None and best_iter is not None:
        best_ntree_limit = best_iter + 1  # íŠ¸ë¦¬ ê°œìˆ˜ëŠ” iteration+1ë¡œ ë³´ëŠ”ê²Œ ì¼ë°˜ì 

    # best_score ì¶”ì¶œ
    best_score = None
    try:
        # evals_result êµ¬ì¡°: {"valid": {"mlogloss": [ ... ]}}
        valid_hist = evals_result.get("valid", {}).get("mlogloss", [])
        if valid_hist:
            best_score = min(valid_hist)
    except Exception:
        pass

    print(f"âœ… {name} | best_iter: {best_iter} | best_ntree_limit: {best_ntree_limit} | best_score(val mlogloss): {best_score}")
    return booster, best_ntree_limit, evals_result

# â–¶ 3-í—¤ë“œ í•™ìŠµ (ì„±ì‹¤ë„/ë³µìŠµí˜•/ì‹œê°„ëŒ€)
clf_s, ntree_s, res_s = _train_with_train_api(X_tr, ys_tr, X_te, ys_te, len(le_s.classes_), w_s_tr, name="ì„±ì‹¤ë„")
clf_r, ntree_r, res_r = _train_with_train_api(X_tr, yr_tr, X_te, yr_te, len(le_r.classes_), w_r_tr, name="ë³µìŠµí˜•")
clf_t, ntree_t, res_t = _train_with_train_api(X_tr, yt_tr, X_te, yt_te, len(le_t.classes_), w_t_tr, name="ì‹œê°„ëŒ€")

# === í•™ìŠµ ê³¡ì„ (mlogloss) ì‹œê°í™” ===
def plot_learning_curve_from_result(evals_result, title):
    vals = evals_result.get("valid", {}).get("mlogloss", None)
    if not vals:
        print(f"[WARN] {title}: validation mlogloss ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    plt.figure()
    plt.plot(vals)
    plt.title(f"{title} mlogloss (val)")
    plt.xlabel("iteration")
    plt.ylabel("mlogloss")
    plt.grid(True)
    plt.show()

plot_learning_curve_from_result(res_s, "ì„±ì‹¤ë„")
plot_learning_curve_from_result(res_r, "ë³µìŠµí˜•")
plot_learning_curve_from_result(res_t, "ì‹œê°„ëŒ€")

# === (ì˜µì…˜) ì˜ˆì¸¡ ë³´ì¡° í•¨ìˆ˜ ===
def _predict_classes(booster, X, num_class, ntree_limit=None):
    """
    booster.predictë¡œ í™•ë¥  ì–»ê³  argmax â†’ ë¼ë²¨ ì¸ë±ìŠ¤ ë°˜í™˜
    """
    dm = xgb.DMatrix(X)
    proba = booster.predict(dm, ntree_limit=ntree_limit)  # shape: (N, num_class)
    # ì¼ë¶€ êµ¬ë²„ì „ì—ì„  2Dê°€ ì•„ë‹Œ 1Dë¡œ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ì§€ë§Œ, ì•ˆì „ì¥ì¹˜:
    proba = np.asarray(proba).reshape(-1, num_class)
    return proba.argmax(axis=1)

"""4) í‰ê°€: ì •í™•ë„/F1, ë¦¬í¬íŠ¸, í˜¼ë™í–‰ë ¬, ì¤‘ìš”ë„(ìƒìœ„ 20)"""

# ============================
# ğŸ”µ ë¸”ëŸ­ 4 â€” í‰ê°€ & ì‹œê°í™” (ë²„ì „ í˜¸í™˜)
# ============================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import xgboost as xgb

# (êµ¬)xgboostì—ì„œ predictê°€ DMatrixë§Œ ë°›ëŠ” ê²½ìš° ëŒ€ë¹„
def _predict_compat(model, X):
    """
    - ìµœì‹  ë²„ì „: model.predict(np.ndarray) ì •ìƒ ë™ì‘
    - êµ¬ë²„ì „: TypeError ë°œìƒ â†’ DMatrixë¡œ ë³€í™˜í•´ì„œ ì¬ì‹œë„
    - ë°˜í™˜ì´ í™•ë¥ (2D)ì¸ ê²½ìš° argmaxë¡œ ë¼ë²¨ ë³€í™˜
    """
    try:
        y_pred = model.predict(X)
    except TypeError:
        # êµ¬ë²„ì „ í˜¸í™˜: DMatrixë¡œ ê°ì‹¸ì„œ ì˜ˆì¸¡
        dm = xgb.DMatrix(X)
        y_pred = model.predict(dm)

    # multi:softprobì—ì„œ booster.predictë¥¼ ì“´ ê²½ìš° 2D í™•ë¥ ì¼ ìˆ˜ ìˆìŒ
    if isinstance(y_pred, np.ndarray) and y_pred.ndim == 2:
        y_pred = np.argmax(y_pred, axis=1)
    return y_pred

def eval_head(model, X_te, y_te, le, title):
    y_pred = _predict_compat(model, X_te)

    acc = accuracy_score(y_te, y_pred)
    f1m = f1_score(y_te, y_pred, average="macro")
    print(f"[{title}] acc={acc:.4f}, f1(macro)={f1m:.4f}\n")
    print(classification_report(y_te, y_pred, target_names=le.classes_))

    # í˜¼ë™í–‰ë ¬
    cm = confusion_matrix(y_te, y_pred, labels=range(len(le.classes_)))
    plt.figure(figsize=(5.2, 4.6))
    plt.imshow(cm, interpolation='nearest', cmap='Blues')
    plt.title(f"{title} Confusion Matrix")
    plt.xticks(range(len(le.classes_)), le.classes_, rotation=45, ha='right')
    plt.yticks(range(len(le.classes_)), le.classes_)
    plt.colorbar()
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i, j], ha="center", va="center", fontsize=10)
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.show()

    return y_pred

# â”€â”€ 3ê°œ í—¤ë“œ í‰ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ps = eval_head(clf_s, X_te, ys_te, le_s, "ì„±ì‹¤ë„")
pr = eval_head(clf_r, X_te, yr_te, le_r, "ë³µìŠµí˜•")
pt = eval_head(clf_t, X_te, yt_te, le_t, "ì‹œê°„ëŒ€")

# â”€â”€ ì¤‘ìš”ë„(ì„±ì‹¤ë„ í—¤ë“œ, ìƒìœ„ 20ê°œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def plot_importance_topk(model, topk=20, title="Feature Importance (ì„±ì‹¤ë„ ìƒìœ„ 20)"):
    """
    ìš°ì„  sklearn ë˜í¼ì˜ feature_importances_ ì‚¬ìš©.
    ë¶ˆê°€í•˜ë©´ boosterì˜ get_score(importance_type='gain')ë¥¼ ì‚¬ìš©(í‚¤ f0,f1,...).
    """
    imp = None
    # 1) sklearn ë˜í¼ ê²½ë¡œ
    if hasattr(model, "feature_importances_"):
        imp = model.feature_importances_
        idx = np.argsort(imp)[::-1][:topk]
        vals = imp[idx]
        labels = [f"f{int(i)}" for i in idx]
    else:
        # 2) booster ê²½ë¡œ
        try:
            booster = model.get_booster()
            score_dict = booster.get_score(importance_type='gain')  # {'f0':gain, ...}
            if not score_dict:
                print("âš ï¸ ì¤‘ìš”ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            # dict â†’ ì •ë ¬
            items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)[:topk]
            labels = [k for k, _ in items]
            vals = [v for _, v in items]
        except Exception:
            print("âš ï¸ ì¤‘ìš”ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    plt.figure(figsize=(9, 4.6))
    plt.bar(range(len(vals)), vals)
    plt.xticks(range(len(vals)), labels, rotation=45, ha='right')
    plt.title(title)
    plt.ylabel("gain")
    plt.grid(True, axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

plot_importance_topk(clf_s, topk=20, title="Feature Importance (ì„±ì‹¤ë„ ìƒìœ„ 20)")


# ====== Booster ì¤‘ìš”ë„ ì‹œê°í™” ìœ í‹¸ ======

import numpy as np
import matplotlib.pyplot as plt

# (ì„ íƒ) í•œê¸€ í°íŠ¸ ê²½ê³  ì œê±° â€“ Colab ê¸°ë³¸ í°íŠ¸ ëŒ€ì²´
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'  # ê²½ê³ ë§Œ ì¤„ì´ê³ , ë ˆì´ë¸”ì€ ì˜ë¬¸/ê¸°í˜¸ ìœ„ì£¼ë¡œ í‘œì‹œ
# ë§Œì•½ ì§„ì§œ í•œê¸€ í°íŠ¸ê°€ í•„ìš”í•˜ë©´:
# !apt-get -y install fonts-nanum
# import matplotlib.font_manager as fm
# fm._rebuild()
# plt.rcParams['font.family'] = 'NanumGothic'

# 1) í‰íƒ„í™” ì¸ë±ìŠ¤ â†’ ì‚¬ëŒì´ ì½ëŠ” ì´ë¦„ ë§Œë“¤ê¸°
base_feats  = ['ì´','ì˜¤ì „','ì˜¤í›„','ì €ë…','ì‹¬ì•¼','ë°˜ë³µ','ë‹¬ì„±']           # 7ê°œ
delta_feats = ['Î”ì´','Î”ì˜¤ì „','Î”ì˜¤í›„','Î”ì €ë…','Î”ì‹¬ì•¼','Î”ë°˜ë³µ','Î”ë‹¬ì„±']    # 7ê°œ
day_feats   = base_feats + delta_feats + ['sin','cos']                  # 16ê°œ

day_names = ['D0(ì›”)','D1(í™”)','D2(ìˆ˜)','D3(ëª©)','D4(ê¸ˆ)','D5(í† )','D6(ì¼)']

def flat_index_to_name(fid: int) -> str:
    d = fid // 16   # day index (0..6)
    f = fid % 16    # feature within day (0..15)
    return f"{day_names[d]}:{day_feats[f]}"

def booster_importance_bar(booster, top_k=20, title="Feature Importance (gain)"):
    # 2) Boosterì˜ get_scoreë¡œ ì¤‘ìš”ë„ ê°€ì ¸ì˜¤ê¸° (gain/weight/cover ë“± ì„ íƒ ê°€ëŠ¥)
    #    í‚¤ëŠ” 'f0','f1',... í˜•íƒœê±°ë‚˜, feature_namesë¥¼ ì§€ì •í–ˆìœ¼ë©´ ê·¸ ì´ë¦„ìœ¼ë¡œ ì˜´.
    score_dict = booster.get_score(importance_type='gain')

    if not score_dict:
        print("âš ï¸ ëª¨ë¸ì—ì„œ ì¤‘ìš”ë„ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. (íŠ¸ë¦¬ê°€ ê±°ì˜ ì•ˆìƒê²¼ê±°ë‚˜, Boosterê°€ ë§ëŠ”ì§€ í™•ì¸ í•„ìš”)")
        return

    # 3) í‚¤ë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ íŒŒì‹± (ì˜ˆ: 'f37' -> 37)
    idxs, gains = [], []
    for k, v in score_dict.items():
        if k.startswith('f') and k[1:].isdigit():
            idxs.append(int(k[1:]))
            gains.append(float(v))
        else:
            # í˜¹ì‹œ ì´ë¦„ì´ ë¬¸ìì—´ë¡œ ë“¤ì–´ì™”ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë³´ì´ê²Œë§Œ ì²˜ë¦¬
            # (ì´ë²ˆ íŒŒì´í”„ë¼ì¸ì—ì„  'fN' í˜•íƒœì¼ ê²ƒ)
            pass

    if not idxs:
        print("âš ï¸ ì¤‘ìš”ë„ í‚¤ê°€ 'f0' í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. feature_namesë¥¼ ì§€ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        # ê·¸ë˜ë„ ê·¸ë ¤ë³´ê¸°
        items = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)[:top_k]
        labels = [k for k, _ in items]
        vals   = [v for _, v in items]
        plt.figure(figsize=(9,4.8))
        plt.bar(range(len(vals)), vals)
        plt.xticks(range(len(vals)), labels, rotation=60, ha='right')
        plt.title(title)
        plt.grid(True, axis='y')
        plt.tight_layout(); plt.show()
        return

    idxs = np.array(idxs); gains = np.array(gains)

    # 4) ìƒìœ„ top_k ê³ ë¥´ê¸°
    order = np.argsort(gains)[::-1]
    top = order[:top_k]
    top_idxs  = idxs[top]
    top_gains = gains[top]
    top_names = [flat_index_to_name(i) for i in top_idxs]

    # 5) ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10,5))
    plt.bar(range(len(top_gains)), top_gains)
    plt.xticks(range(len(top_gains)), top_names, rotation=60, ha='right')
    plt.title(title)
    plt.ylabel('gain')
    plt.grid(True, axis='y')
    plt.tight_layout()
    plt.show()

# === í˜¸ì¶œì˜ˆ: ê° í—¤ë“œì˜ Boosterë¥¼ ë„˜ê²¨ì£¼ì„¸ìš” ===
#  - ë°©ë²•1(xgb.train)ë¡œ í•™ìŠµí–ˆë‹¤ë©´ clf_* ìì²´ê°€ Booster ì…ë‹ˆë‹¤.
booster_importance_bar(clf_s, top_k=20, title="ì„±ì‹¤ë„ ì¤‘ìš”ë„ (gain)")
booster_importance_bar(clf_r, top_k=20, title="ë³µìŠµí˜• ì¤‘ìš”ë„ (gain)")
booster_importance_bar(clf_t, top_k=20, title="ì‹œê°„ëŒ€ ì¤‘ìš”ë„ (gain)")

"""5) ì €ì¥ + ì¶”ë¡  í•¨ìˆ˜ (ì˜µì…˜)"""

import xgboost as xgb
import numpy as np

def _predict_classes_booster(booster, X2d, num_class,
                             end_iter=None,  # ìƒˆ APIìš© (iteration_range)
                             ntree_limit=None):  # êµ¬ APIìš©
    """
    - XGBoost ë²„ì „ë³„ í˜¸í™˜:
      1) iteration_range=(0, end_iter) ì‹œë„
      2) ì‹¤íŒ¨í•˜ë©´ ntree_limit=end_iter ì‹œë„
      3) ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ntree_limit íŒŒë¼ë¯¸í„° ì‚¬ìš© or ì œí•œ ì—†ì´ ì˜ˆì¸¡
    """
    dm = xgb.DMatrix(X2d)

    # 1) ìƒˆ API ë¨¼ì € ì‹œë„
    if end_iter is not None:
        try:
            proba = booster.predict(dm, iteration_range=(0, int(end_iter)))
        except TypeError:
            # 2) êµ¬ API ëŒ€ì²´ ì‹œë„
            try:
                proba = booster.predict(dm, ntree_limit=int(end_iter))
            except TypeError:
                proba = booster.predict(dm)
    else:
        # end_iterê°€ ì—†ìœ¼ë©´ ntree_limitë¡œ ì‹œë„
        if ntree_limit is not None:
            try:
                proba = booster.predict(dm, ntree_limit=int(ntree_limit))
            except TypeError:
                try:
                    proba = booster.predict(dm, iteration_range=(0, int(ntree_limit)))
                except TypeError:
                    proba = booster.predict(dm)
        else:
            proba = booster.predict(dm)

    proba = np.asarray(proba).reshape(-1, num_class)
    return proba.argmax(axis=1)

def predict_user_type_xgb(week_7x7,
                          booster_s=clf_s, booster_r=clf_r, booster_t=clf_t,
                          scaler16=scaler, le_s=le_s, le_r=le_r, le_t=le_t,
                          meta=meta if 'meta' in globals() else None):
    """
    week_7x7: 7ì¼ x 7íŠ¹ì§• [ì´, ì˜¤ì „, ì˜¤í›„, ì €ë…, ì‹¬ì•¼, ë°˜ë³µíšŸìˆ˜, ì¼ì¼ë‹¬ì„±ë¥ ]
    - Î” + ìš”ì¼ sin/cos â†’ ìŠ¤ì¼€ì¼ â†’ (1,112)
    - Booster ì˜ˆì¸¡: ë²„ì „ì— ë§ì¶° iteration_range ë˜ëŠ” ntree_limit ìë™ ì‚¬ìš©
    """
    arr = np.array(week_7x7, dtype=np.float32).reshape(7,7)

    # Î” + sin/cos â†’ (7,16)
    diff  = np.diff(arr, axis=0, prepend=arr[:1,:])
    arr14 = np.concatenate([arr, diff], axis=1)
    dow = np.arange(7)
    sincos = np.stack([np.sin(2*np.pi*dow/7), np.cos(2*np.pi*dow/7)], axis=1)
    arr16 = np.concatenate([arr14, sincos], axis=1)

    # ìŠ¤ì¼€ì¼ & í‰íƒ„í™”
    flat = scaler16.transform(arr16.reshape(-1, arr16.shape[1])).reshape(1, -1)  # (1,112)

    # ë©”íƒ€ì—ì„œ best iteration/ntree_limit ì½ê¸°
    end_s = end_r = end_t = None
    nls = nlr = nlt = None
    if meta is not None:
        # ìƒˆ í˜•ì‹: meta["best_iter"][...], êµ¬ í˜•ì‹: meta["ntree_limit"][...]
        bi = meta.get("best_iter", {})
        nl = meta.get("ntree_limit", {})
        end_s = bi.get("sincerity", None) or nl.get("sincerity", None)
        end_r = bi.get("repetition", None) or nl.get("repetition", None)
        end_t = bi.get("timeslot",  None) or nl.get("timeslot",  None)
        # í˜¸í™˜ ìœ„í•´ ntree_limit ê°’ë„ ë“¤ê³  ìˆìŒ
        nls = nl.get("sincerity", None); nlr = nl.get("repetition", None); nlt = nl.get("timeslot", None)

    s_idx = int(_predict_classes_booster(booster_s, flat, len(le_s.classes_),
                                         end_iter=end_s, ntree_limit=nls)[0])
    r_idx = int(_predict_classes_booster(booster_r, flat, len(le_r.classes_),
                                         end_iter=end_r, ntree_limit=nlr)[0])
    t_idx = int(_predict_classes_booster(booster_t, flat, len(le_t.classes_),
                                         end_iter=end_t, ntree_limit=nlt)[0])

    return {
        "ì„±ì‹¤ë„": le_s.inverse_transform([s_idx])[0],
        "ë°˜ë³µí˜•": le_r.inverse_transform([r_idx])[0],
        "ì‹œê°„ëŒ€": le_t.inverse_transform([t_idx])[0],
    }

# ===== Block 6. ë¡œë“œ & ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ =====
import os, json, pickle, joblib
import numpy as np
import xgboost as xgb

# â–¶ í›ˆë ¨ ë•Œ ì €ì¥í•œ ê²½ë¡œì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
SAVE_DIR = "/content/trained_models_xgb"  # í•„ìš”ì‹œ ë³€ê²½

# -----------------------------
# 1) ì•„í‹°íŒ©íŠ¸ ë¡œë”
# -----------------------------
def load_all(save_dir: str):
    """
    ë°˜í™˜:
      - clf_s, clf_r, clf_t: XGBClassifier ë˜ëŠ” xgb.Booster ê°ì²´(ë‘˜ ì¤‘ ë¬´ì—‡ì´ë“  OK)
      - scaler: MinMaxScaler (7â†’14â†’16 ì „ì²˜ë¦¬ í›„ 16ì°¨ì›ì— ëŒ€í•´ í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬)
      - enc: {"sincerity": LabelEncoder, "repetition": LabelEncoder, "timeslot": LabelEncoder}
      - meta: {"ntree_limit": {...}} ë˜ëŠ” {}
    """
    # ìŠ¤ì¼€ì¼ëŸ¬
    scaler = joblib.load(os.path.join(save_dir, "scaler16.joblib"))

    # ë¼ë²¨ ì¸ì½”ë”
    with open(os.path.join(save_dir, "label_encoders.pkl"), "rb") as f:
        enc = pickle.load(f)

    # ë©”íƒ€ (ì—†ì–´ë„ ë™ì‘)
    meta_path = os.path.join(save_dir, "meta.json")
    meta = {}
    if os.path.exists(meta_path):
        with open(meta_path, "r") as f:
            meta = json.load(f)

    # ë¶„ë¥˜ê¸°(XGBClassifier) ë¨¼ì € ì‹œë„, ì—†ìœ¼ë©´ Booster ë¡œë“œ
    def _load_head(prefix):
        cls_path = os.path.join(save_dir, f"{prefix}.joblib")
        bst_path = os.path.join(save_dir, f"{prefix}.booster")
        if os.path.exists(cls_path):
            return joblib.load(cls_path)
        elif os.path.exists(bst_path):
            bst = xgb.Booster()
            bst.load_model(bst_path)
            return bst
        else:
            raise FileNotFoundError(f"{prefix} ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cls_path} / {bst_path}")

    clf_s = _load_head("xgb_sincerity")
    clf_r = _load_head("xgb_repetition")
    clf_t = _load_head("xgb_timeslot")

    return clf_s, clf_r, clf_t, scaler, enc, meta

# -----------------------------
# 2) 7x7 â†’ 7x16 ì „ì²˜ë¦¬ (í›ˆë ¨ ë•Œì™€ ë™ì¼)
# -----------------------------
def _to_16_features(week_7x7: np.ndarray) -> np.ndarray:
    """
    ì…ë ¥: (7,7) [ì´,ì˜¤ì „,ì˜¤í›„,ì €ë…,ì‹¬ì•¼,ë°˜ë³µ,ë‹¬ì„±ë¥ ]
    ì¶œë ¥: (7,16) = ì›ë³¸7 + Î”7 + ìš”ì¼ sin/cos(2)
    """
    arr = np.array(week_7x7, dtype=np.float32).reshape(7, 7)
    diff = np.diff(arr, axis=0, prepend=arr[:1, :])           # (7,7)
    arr14 = np.concatenate([arr, diff], axis=1)               # (7,14)
    dow = np.arange(7)
    sincos = np.stack([np.sin(2*np.pi*dow/7), np.cos(2*np.pi*dow/7)], axis=1)  # (7,2)
    arr16 = np.concatenate([arr14, sincos], axis=1)           # (7,16)
    return arr16

# -----------------------------
# 3) ë¶„ë¥˜ê¸°/ë¶€ìŠ¤í„° í˜¸í™˜ ì˜ˆì¸¡ í—¬í¼
# -----------------------------
def _predict_head(model, flat_112: np.ndarray, n_classes: int, ntree_limit: int | None = None):
    """
    model: XGBClassifier ë˜ëŠ” xgb.Booster
    flat_112: (1, 112)
    n_classes: í´ë˜ìŠ¤ ìˆ˜
    ntree_limit: Early Stopping ìµœì  íŠ¸ë¦¬ ìˆ˜ (ì—†ìœ¼ë©´ ì „ì²´ ì‚¬ìš©)

    ë°˜í™˜: argmax(class) ì •ìˆ˜ id
    """
    if isinstance(model, xgb.Booster):
        dtest = xgb.DMatrix(flat_112)
        # ìµœì‹ /êµ¬ë²„ì „ í˜¸í™˜: ntree_limit ëŒ€ì‹  iteration_range ì‚¬ìš©
        # ntree_limitê°€ ì—†ìœ¼ë©´ ì „ì²´ íŠ¸ë¦¬ ì‚¬ìš©
        if ntree_limit and ntree_limit > 0:
            # iteration_rangeëŠ” [start, end) ì´ë¯€ë¡œ end=ntree_limit ë¡œ ì£¼ë©´ ntree_limitê°œ ì‚¬ìš©
            prob = model.predict(dtest, iteration_range=(0, ntree_limit))
        else:
            prob = model.predict(dtest)
        # multi:softprob â†’ (1, n_classes)
        cls_id = int(np.argmax(prob.reshape(-1, n_classes), axis=1)[0])
        return cls_id
    else:
        # XGBClassifier
        prob = model.predict_proba(flat_112)
        cls_id = int(np.argmax(prob.reshape(-1, n_classes), axis=1)[0])
        return cls_id

# -----------------------------
# 4) ì¼ê´„ ì˜ˆì¸¡ í•¨ìˆ˜ (ì‚¬ì „/ì‚¬í›„ ì¸ì½”ë”© í¬í•¨)
# -----------------------------
def predict_user_type_xgb(
    week_7x7,
    booster_s=None, booster_r=None, booster_t=None,
    scaler16=None, le_s=None, le_r=None, le_t=None,
    ntree_s: int | None = None, ntree_r: int | None = None, ntree_t: int | None = None,
):
    """
    ê¸°ë³¸ì ìœ¼ë¡œ SAVE_DIRì—ì„œ ë¡œë“œëœ ì•„í‹°íŒ©íŠ¸ë¥¼ ì‚¬ìš©.
    í•„ìš” ì‹œ ì¸ìë¡œ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/ì¸ì½”ë”/ntree ì •ë³´ë¥¼ ì§ì ‘ ì£¼ì…í•´ë„ OK.
    """
    # ë¡œì»¬ ì•„í‹°íŒ©íŠ¸ ìë™ ë¡œë“œ
    if any(v is None for v in [booster_s, booster_r, booster_t, scaler16, le_s, le_r, le_t]):
        booster_s, booster_r, booster_t, scaler16, enc, meta = load_all(SAVE_DIR)
        le_s, le_r, le_t = enc["sincerity"], enc["repetition"], enc["timeslot"]
        # ë©”íƒ€ì—ì„œ ìµœì  íŠ¸ë¦¬ ìˆ˜ ìˆìœ¼ë©´ ë°˜ì˜
        nmeta = meta.get("ntree_limit", {}) if isinstance(meta, dict) else {}
        ntree_s = ntree_s or nmeta.get("sincerity", None)
        ntree_r = ntree_r or nmeta.get("repetition", None)
        ntree_t = ntree_t or nmeta.get("timeslot", None)

    # 7x7 â†’ 7x16 â†’ ìŠ¤ì¼€ì¼ â†’ (1,112)
    arr16 = _to_16_features(week_7x7)
    flat = scaler16.transform(arr16.reshape(-1, arr16.shape[1])).reshape(1, -1)

    # 3í—¤ë“œ ì˜ˆì¸¡
    sid = _predict_head(booster_s, flat, n_classes=len(le_s.classes_), ntree_limit=ntree_s)
    rid = _predict_head(booster_r, flat, n_classes=len(le_r.classes_), ntree_limit=ntree_r)
    tid = _predict_head(booster_t, flat, n_classes=len(le_t.classes_), ntree_limit=ntree_t)

    s_lab = le_s.inverse_transform([sid])[0]
    r_lab = le_r.inverse_transform([rid])[0]
    t_lab = le_t.inverse_transform([tid])[0]
    return {"ì„±ì‹¤ë„": s_lab, "ë°˜ë³µí˜•": r_lab, "ì‹œê°„ëŒ€": t_lab}

# -----------------------------
# 5) ìƒ˜í”Œ ìƒì„±ê¸°(ê°„ë‹¨) & í…ŒìŠ¤íŠ¸
# -----------------------------
def gen_sample_week(timeslot="ì˜¤ì „", total_mu=130, total_sigma=40, ach_mu=70, ach_sigma=15, zero_day_p=0.15):
    """
    ê°„ë‹¨ ìƒ˜í”ŒëŸ¬: ì§€ì •í•œ ì£¼ ì§‘ì¤‘ ì‹œê°„ëŒ€ì— ë¹„ì¤‘ì„ ë” ì¤˜ì„œ 7x7 ì£¼ ë°ì´í„°ë¥¼ ìƒì„±
    """
    rows = []
    for d in range(7):
        # ì´ í•™ìŠµì‹œê°„
        tot = max(0, int(np.random.normal(total_mu, total_sigma)))
        if np.random.rand() < zero_day_p:
            tot = 0
        # ë‹¬ì„±ë¥ /ë°˜ë³µ
        rep = 0 if tot == 0 else int(max(0, np.random.poisson(lam=1.2)))
        ach = int(np.clip(np.random.normal(ach_mu, ach_sigma), 0, 100))

        if tot == 0:
            m=a=e=n=0
        else:
            if timeslot == "ì˜¤ì „":
                m_ratio = np.random.uniform(0.55, 0.85)
                a_ratio = np.random.uniform(0.05, 0.25)
                e_ratio = np.random.uniform(0.05, 0.25)
            elif timeslot == "ì˜¤í›„":
                a_ratio = np.random.uniform(0.55, 0.85)
                m_ratio = np.random.uniform(0.05, 0.25)
                e_ratio = np.random.uniform(0.05, 0.25)
            elif timeslot == "ì €ë…":
                e_ratio = np.random.uniform(0.55, 0.85)
                m_ratio = np.random.uniform(0.05, 0.25)
                a_ratio = np.random.uniform(0.05, 0.25)
            else:  # ì‹¬ì•¼
                m_ratio = np.random.uniform(0.01, 0.15)
                a_ratio = np.random.uniform(0.01, 0.15)
                e_ratio = np.random.uniform(0.01, 0.15)
            s = m_ratio + a_ratio + e_ratio
            m = int(tot * (m_ratio / (s + 1e-6)))
            a = int(tot * (a_ratio / (s + 1e-6)))
            e = int(tot * (e_ratio / (s + 1e-6)))
            n = max(0, tot - m - a - e)

        rows.append([tot, m, a, e, n, rep, ach])
    return rows

# â”€â”€ ì €ì¥ë¬¼ ë¡œë“œ(í•œ ë²ˆë§Œ)
clf_s, clf_r, clf_t, scaler, enc, meta = load_all(SAVE_DIR)
le_s, le_r, le_t = enc["sincerity"], enc["repetition"], enc["timeslot"]
ntree = meta.get("ntree_limit", {}) if isinstance(meta, dict) else {}
print("Loaded. ntree_limit:", ntree)

# â”€â”€ â‘  ë¬¸ì„œ ì˜ˆì œì™€ ìœ ì‚¬í•œ ìˆ˜ë™ ìƒ˜í”Œ
# example = [
#     [300, 50, 200, 40, 10, 2, 100],
#     [300, 20, 240, 30, 10, 1, 60],
#     [180, 60, 60, 50, 10, 2, 100],
#     [0,   0,  0,  0,  0, 0,  0],
#     [300,  10, 250, 30, 10, 2, 100],
#     [140, 40, 50, 40, 10, 1, 100],
#     [130, 20, 70, 30, 10, 2, 100],
# ]
example = [
    [0, 0, 0, 0, 0, 2, 0],
    [0, 0, 0, 0, 0, 1, 0],
    [180, 60, 60, 50, 10, 2, 100],
    [0,   0,  0,  0,  0, 0,  0],
    [0,  0, 0, 0, 0, 2, 0],
    [0, 0, 0, 0, 0, 1, 0],
    [0, 0, 0, 0, 0, 2, 0],
]
print("ìƒ˜í”Œ ì˜ˆì¸¡(ìˆ˜ë™):",
      predict_user_type_xgb(
          example,
          booster_s=clf_s, booster_r=clf_r, booster_t=clf_t,
          scaler16=scaler, le_s=le_s, le_r=le_r, le_t=le_t,
          ntree_s=ntree.get("sincerity"), ntree_r=ntree.get("repetition"), ntree_t=ntree.get("timeslot"))
)

# â”€â”€ â‘¡ ëœë¤ ìƒ˜í”Œ(ì‹œê°„ëŒ€ë³„ 1ê°œì”©)
for ts in ["ì˜¤ì „", "ì˜¤í›„", "ì €ë…", "ì‹¬ì•¼"]:
    sample = gen_sample_week(timeslot=ts)
    pred = predict_user_type_xgb(
        sample,
        booster_s=clf_s, booster_r=clf_r, booster_t=clf_t,
        scaler16=scaler, le_s=le_s, le_r=le_r, le_t=le_t,
        ntree_s=ntree.get("sincerity"), ntree_r=ntree.get("repetition"), ntree_t=ntree.get("timeslot"))
    print(f"[ëœë¤ {ts}] â†’ {pred}")