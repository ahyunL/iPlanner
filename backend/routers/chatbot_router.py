# backend/routers/chatbot_router.py

from fastapi import APIRouter, Request
from schemas.chatbot_schema import ChatRequest
from services.rag_service import retrieve_relevant_chunks
from services.gpt_service import ask_gpt

router = APIRouter()

@router.post("/chat")
def chat_with_gpt(request: ChatRequest):
    try:
        chunks = retrieve_relevant_chunks(request.question)

        # âœ… ë¬¸ë§¥ ì—¬ë¶€ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ë‹¤ë¥´ê²Œ êµ¬ì„±
        if not chunks:
            # ğŸ” GPT only, but ì •ì§í•˜ê²Œ (ë„ˆê°€ ì›í•˜ëŠ” fallback)
            prompt = f"""
ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìê°€ ì˜¬ë¦° ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 
ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ë²”ìœ„ì—ì„œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

[ì§ˆë¬¸]
{request.question}

[ë‹µë³€]
"""
        else:
            # âœ… RAG + GPT ê²°í•© ì‘ë‹µ
            context = "\n".join(chunks)
            prompt = f"""
ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

[ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{request.question}

[ë‹µë³€]
"""

        answer = ask_gpt(prompt)
        return {"answer": answer}

    except Exception as e:
        return {"error": str(e)}
